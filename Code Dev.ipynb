{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7b393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 17:57:03.684211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760389023.701024    9340 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760389023.706085    9340 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760389023.719552    9340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760389023.719579    9340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760389023.719582    9340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760389023.719585    9340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-13 17:57:03.723675: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "from keras import layers, initializers, optimizers, losses\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22172881",
   "metadata": {},
   "source": [
    "In order for we to not have to import tensorflow every time we want to test something. Development for the models will be made here and then migrated to the proper .py files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "85583418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrailtyModelNN(keras.models.Model):\n",
    "\n",
    "    def __init__(self, parameters, loglikelihood, input_dim = None, seed = None):\n",
    "        super().__init__()\n",
    "        self.parameters = {\n",
    "            \"theta\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"nn\", \"update_func\": None, \"shape\": None, \"initializer\": None},\n",
    "            \"tau\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"nn\", \"update_func\": None, \"shape\": 5, \"initializer\": None},\n",
    "            \"alpha\": {\"link\": lambda x : 1/(1+tf.math.exp(-x)), \"link_inv\": lambda x : tf.math.log(x) - tf.math.log(1-x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 0.5},\n",
    "            \"gamma\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 1.0},\n",
    "            \"phi1\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 1.0},\n",
    "            \"phi2\": {\"link\": lambda x : tf.identity(x), \"link_inv\": lambda x : tf.identity(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 0.0}\n",
    "        }\n",
    "        self.n_acum_step = tf.Variable(0, dtype = tf.int32, trainable = False)\n",
    "\n",
    "        if(input_dim is not None):\n",
    "            self.define_structure(input_dim, seed)\n",
    "\n",
    "\n",
    "    def clone_architecture(self, model, input_dim):\n",
    "        pass\n",
    "    \n",
    "    def define_structure(self, input_dim, seed):\n",
    "\n",
    "        # Goes through the list of parameters for the model and filter them by their classes:\n",
    "        # - \"nn\" will be treated as an output from a given neural network that receives the variables x as input.\n",
    "        # - \"independet\" will be treated an an individual tf.Variable, trainable object. It is still trained in tensorflow, but is constant for all subjects\n",
    "        # - \"fixed\" will be treated as a non-trainable tf.Variable. Basically just a known constant.\n",
    "        # - \"manual\" will be treated as a non-trainable tf.Variable, but its value will be eventually updated manually using user provided functions (useful in cases where closed forms can be obtained)\n",
    "        # - \"dependent\" will be treated simply as a deterministic function of other parameters and will be updated after training\n",
    "\n",
    "        self.nn_pars = []\n",
    "        self.independent_pars = []\n",
    "        self.fixed_pars = []\n",
    "        self.manual_pars = []\n",
    "        for parameter in self.parameters:\n",
    "            par = self.parameters[parameter]\n",
    "            if(par[\"par_type\"] == \"nn\"):\n",
    "                self.nn_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"independent\"):\n",
    "                self.independent_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"fixed\"):\n",
    "                self.fixed_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"manual\"):\n",
    "                self.manual_pars.append( parameter )\n",
    "            else:\n",
    "                raise Exception(\"Invalid parameter {} type: {}\".format(parameter, par[\"par_type\"]))\n",
    "\n",
    "        # If at least one parameter is to be modeled as a neural network output, define its architecture here\n",
    "        if( len(self.nn_pars) > 0 ):\n",
    "            # Try to implement a function that, given a model, clone its architecture\n",
    "            # clone_architecture(given_model)\n",
    "\n",
    "            # The user provided architecture must output an array with the exact same shape as the concatenation of all \"nn\" parameters.\n",
    "            # For example, if theta (nn) is a single constant and alpha (nn) is an array with 3 values, the expected output for the neural network\n",
    "            # component is 4. For that, these parameters must be at most one dimensional arrays. (No matrix, or more complex structured parameters!)\n",
    "            initializer = initializers.GlorotNormal(seed = seed)\n",
    "            self.dense1 = keras.layers.Dense(units = 16, activation = \"gelu\", kernel_initializer = initializer, dtype = tf.float32, name = \"dense1\")\n",
    "            self.dense2 = keras.layers.Dense(units = 6, kernel_initializer = initializer, dtype = tf.float32, activation = None, use_bias = False, name = \"output\")\n",
    "\n",
    "        # Dictionary with all parameters that are its individual weights\n",
    "        self.model_variables = {}\n",
    "\n",
    "        # Include variables that do not depend on the variables x, but are still trained by tensorflow\n",
    "        for parameter in self.independent_pars:\n",
    "            par = self.parameters[parameter]\n",
    "\n",
    "            # If shape is None, set it to ()\n",
    "            if(par[\"shape\"] is None):\n",
    "                par[\"shape\"] = ()\n",
    "\n",
    "            raw_init = par[\"link_inv\"]( par[\"init\"] )\n",
    "\n",
    "            # Name for the new, transformed parameter\n",
    "            raw_parameter = \"raw_\" + parameter\n",
    "            self.model_variables[raw_parameter] = self.add_weight(\n",
    "                name = raw_parameter,\n",
    "                shape = par[\"shape\"],\n",
    "                initializer = keras.initializers.Constant( raw_init ),\n",
    "                trainable = True,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "\n",
    "        # Include variables that are not trained by tensorflow (known, fixed constants or manual trained variables)\n",
    "        for parameter in np.concatenate([self.fixed_pars, self.manual_pars]):\n",
    "            par = self.parameters[parameter]\n",
    "            self.model_variables[parameter] = self.add_weight(\n",
    "                name = parameter,\n",
    "                shape = par[\"shape\"],\n",
    "                initializer = keras.initializers.Constant( par[\"initializer\"] ),\n",
    "                trainable = False,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "\n",
    "        # Organize trainable variables information, so each variable can get mapped to an index in the self.trainable_variables and its gradients\n",
    "        self.vars_to_index = {}\n",
    "        # Before we build the model, the only variables that appear in here are the ones corresponding to \"independent\" parameters\n",
    "        for i, var in enumerate(self.trainable_variables):\n",
    "            # From the variable path, get its name (raw_<variable>)\n",
    "            var_name = var.path.split(\"/\")[-1]\n",
    "            # Save its corresponding index\n",
    "            self.vars_to_index[var_name] = i\n",
    "\n",
    "        nn_par_index = 0\n",
    "        # We must also include in this list the indices for \"nn\" parameters\n",
    "        for i, parameter in enumerate(self.nn_pars):\n",
    "            par = self.parameters[ parameter ]\n",
    "            if(par[\"shape\"] is None):\n",
    "                par_shape = 1\n",
    "            else:\n",
    "                # The parameter must be at most a 1-dimensional array, whose indices will be saved for future location in the neural network output results\n",
    "                par_shape = par[\"shape\"]\n",
    "            \n",
    "            self.vars_to_index[\"raw_\" + parameter] = tf.constant( np.arange(nn_par_index, nn_par_index+par_shape), dtype = tf.int32 )\n",
    "            nn_par_index += par_shape\n",
    "\n",
    "\n",
    "        # Once the entire structure has been defined, force the model to build all the weights properly\n",
    "        dummy_input = keras.Input(input_dim)\n",
    "        self(dummy_input)\n",
    "\n",
    "\n",
    "        # In the future, it might be interesting to allow the user to specify an optimizer for each single parameter in the model.\n",
    "        # For now, they will specify one for the independent parameters and other for the neural network weights\n",
    "\n",
    "        # Not that the model is built and all the trainable variables instantiated, we define the gradient variables\n",
    "        self.gradient_accumulation_independent_pars = [\n",
    "            tf.Variable(tf.zeros_like(v, dtype = tf.float32), trainable = False) for v in self.trainable_variables[ :len(self.independent_pars) ]\n",
    "        ]\n",
    "\n",
    "        # The gradient values for the neural network component always comes right after the weights for the independent parameters\n",
    "        self.gradient_accumulation_nn = [\n",
    "            tf.Variable(tf.zeros_like(v, dtype = tf.float32), trainable = False) for v in self.trainable_variables[ len(self.independent_pars): ]\n",
    "        ]\n",
    "\n",
    "\n",
    "    def call(self, x_input):\n",
    "        x = self.dense1(x_input)\n",
    "        return self.dense2(x)\n",
    "\n",
    "    def get_variable(self, parameter, nn_output = None):\n",
    "        \"\"\"\n",
    "            Once that all variables have been properly defined and mapped, this method uses their proper link functions to transform from\n",
    "            the variables 'raw' state into their proper values used in the likelihood.\n",
    "\n",
    "            If nn_output is passed, we automatically assume that the parameter is an output from the neural network and proceed by taking its\n",
    "            value differently than if it was an independent parameter.\n",
    "        \"\"\"\n",
    "        # Get the raw name for that parameter\n",
    "        raw_parameter = \"raw_\" + parameter\n",
    "        # Filter the desired parameter from the list\n",
    "        par = self.parameters[parameter]\n",
    "\n",
    "        # If nn_output is None, assume the parameter is independent from the data x and get it directly as a transformed weight\n",
    "        if(nn_output is None):            \n",
    "            # Get the transformed parameter from its raw version, considering its proper link function\n",
    "            return par[\"link\"]( self.model_variables[raw_parameter] )\n",
    "        \n",
    "        # If nn_output is not None, assume the parameter came as a neural network output and return it from its positions in the output\n",
    "        return par[\"link\"]( tf.gather(nn_output, self.vars_to_index[raw_parameter], axis = 1) )\n",
    "\n",
    "    def loss_func(self, variables, nn_output, t, delta):\n",
    "        \"\"\"\n",
    "            This is an example of a correctly defined loss function.\n",
    "\n",
    "            - eta: Corresponds to the output of the neural network with input x - Corresponds to the \n",
    "        \"\"\"\n",
    "\n",
    "        theta = self.get_variable(\"theta\", nn_output)\n",
    "        tau = self.get_variable(\"theta\", nn_output)\n",
    "\n",
    "        alpha = self.get_variable(\"alpha\")\n",
    "        gamma = self.get_variable(\"gamma\")\n",
    "        phi1 = self.get_variable(\"phi1\")\n",
    "        phi2 = self.get_variable(\"phi2\")\n",
    "\n",
    "        # log_f0 = tf.math.log(phi1) + (phi1-1)*tf.math.log(y) + phi2 - tf.math.exp(phi2) * tf.math.pow(y, phi1)\n",
    "        # F0 = 1 - tf.math.exp(-tf.math.pow(y, phi1) * tf.math.exp(phi2))\n",
    "        # laplace_transform_term = 1 + gamma*theta*F0/(1-alpha)\n",
    "\n",
    "        # loss_weights = delta*eta + delta*log_f0 + (1-alpha)/(alpha*gamma)*(1 - tf.math.pow(laplace_transform_term, alpha)) + (alpha-1)*delta*tf.math.log(laplace_transform_term)\n",
    "        # loss_weights_mean = -tf.math.reduce_mean(loss_weights)\n",
    "        \n",
    "        # return loss_weights_mean\n",
    "\n",
    "        return tf.constant( (phi2 - 3)**2 )\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "            Called by each batch in order to evaluate the loglikelihood and accumulate the parameters gradients using training data.\n",
    "        \"\"\"\n",
    "        x, t, delta = data\n",
    "\n",
    "        self.n_acum_step.assign_add(1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            nn_output = self(x, training = True)\n",
    "            likelihood_loss = self.loss_func(variables = self.model_variables, nn_output = nn_output, t = t, delta = delta)\n",
    "\n",
    "        # The first weights are always destined to the fixed parameters\n",
    "        # The neural network related weights comes after those in the self.trainable_variables object\n",
    "        gradients = tape.gradient(likelihood_loss, self.trainable_variables)\n",
    "\n",
    "        # If the loss does not depend on a specific parameter, its corresponding gradient will be None\n",
    "        # To avoid crash problems in that case, we simply replace None with a zero like gradient, so those weights do not get updated\n",
    "        # It is the user's responsibility to build a loss that depends on all the trainable parameters, but we allow that to happen in this case\n",
    "        # for generality and to avoid unneccessary crashes when testing new models\n",
    "        gradients = [\n",
    "            g if g is not None else tf.zeros_like(v)\n",
    "            for g, v in zip(gradients, self.trainable_variables)\n",
    "        ]\n",
    "\n",
    "        independent_gradients = gradients[ :len(self.independent_pars) ]\n",
    "        nn_gradients = gradients[ len(self.independent_pars): ]\n",
    "\n",
    "        for i in range( len(self.gradient_accumulation_independent_pars) ):\n",
    "            self.gradient_accumulation_independent_pars[i].assign_add( independent_gradients[i] )\n",
    "\n",
    "        for i in range( len(self.gradient_accumulation_nn) ):\n",
    "            self.gradient_accumulation_nn[i].assign_add( nn_gradients[i] )\n",
    "\n",
    "        tf.cond(tf.equal(self.n_acum_step, self.gradient_accumulation_steps), self.apply_accumulated_gradients, lambda: None)\n",
    "\n",
    "        return {\"likelihood_loss\": likelihood_loss}\n",
    "\n",
    "    def apply_accumulated_gradients(self):\n",
    "        # ----------------------------------- Independent parameters component -----------------------------------\n",
    "        # Apply the accumulated gradients to the trainable variables\n",
    "        self.optimizer_independent_pars.apply_gradients( zip(self.gradient_accumulation_independent_pars, self.trainable_variables[ :len(self.independent_pars) ]) )\n",
    "        # Resets all the cumulated gradients to zero\n",
    "        for i in range(len(self.gradient_accumulation_nn)):\n",
    "            self.gradient_accumulation_independent_pars[i].assign(tf.zeros_like(self.trainable_variables[ :len(self.independent_pars) ][i], dtype = tf.float32))\n",
    "        \n",
    "        # ----------------------------------- Neural network component -----------------------------------\n",
    "        self.optimizer_nn.apply_gradients( zip(self.gradient_accumulation_nn, self.trainable_variables[ len(self.independent_pars): ]) )\n",
    "        # Resets all the cumulated gradients to zero\n",
    "        for i in range(len(self.gradient_accumulation_nn)):\n",
    "            self.gradient_accumulation_nn[i].assign(tf.zeros_like(self.trainable_variables[ len(self.independent_pars): ][i], dtype = tf.float32))\n",
    "\n",
    "        # Reset the gradient accumulation steps counter to zero\n",
    "        self.n_acum_step.assign(0)\n",
    "\n",
    "    def compile_model(self, optimizer_independent_pars, optimizer_nn, run_eagerly):\n",
    "        \"\"\"\n",
    "            Defines the configuration for the model, such as batch size, training mode, early stopping.\n",
    "        \"\"\"\n",
    "        # optimizers.Adam(learning_rate = learning_rate, gradient_accumulation_steps = None),\n",
    "        self.optimizer_independent_pars = optimizer_independent_pars\n",
    "        self.optimizer_nn = optimizer_nn\n",
    "        self.compile(\n",
    "            run_eagerly = run_eagerly\n",
    "        )\n",
    "\n",
    "    def train_model(self, x, t, delta,\n",
    "                    epochs, shuffle = True,\n",
    "                    optimizer_independent_pars = optimizers.Adam(learning_rate = 0.001),\n",
    "                    optimizer_nn = optimizers.Adam(learning_rate = 0.001),\n",
    "                    train_batch_size = None, val_batch_size = None,\n",
    "                    buffer_size = 4096, gradient_accumulation_steps = None,\n",
    "                    early_stopping = True, early_stopping_min_delta = 0.0, early_stopping_patience = 10, early_stopping_warmup = 0,\n",
    "                    run_eagerly = True, verbose = 1):\n",
    "        \"\"\"\n",
    "            This is the function that start the training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass the input variables to tensorflow default types\n",
    "        x = tf.cast(x, dtype = tf.float32)\n",
    "        t = tf.cast(t, dtype = tf.float32)\n",
    "        delta = tf.cast(delta, dtype = tf.float32)\n",
    "        \n",
    "        # If input is a vector, transform it into a column\n",
    "        if(len(x.shape) == 1):\n",
    "            x = tf.reshape( x, shape = (len(x), 1) )\n",
    "        if(len(t.shape) == 1):\n",
    "            t = tf.reshape( t, shape = (len(t), 1) )\n",
    "        if(len(delta.shape) == 1):\n",
    "            delta = tf.reshape( delta, shape = (len(delta), 1) )\n",
    "\n",
    "        # Salva os dados originais\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.delta = delta\n",
    "\n",
    "        # If no validation step should be taken, training data is the same as validation data\n",
    "        self.x_train, self.t_train, self.delta_train = self.x, self.t, self.delta\n",
    "        self.x_val, self.t_val, self.delta_val = self.x, self.t, self.delta\n",
    "        \n",
    "        # Declara os callbacks do modelo\n",
    "        self.callbacks = [ ]\n",
    "        \n",
    "        if(verbose >= 1):\n",
    "            self.callbacks.append( TqdmCallback(verbose = 0, position = 0, leave = True) )\n",
    "        \n",
    "        if(early_stopping):\n",
    "            # Avoids overfitting and speeds training\n",
    "            metric = \"likelihood_loss\"\n",
    "            es = keras.callbacks.EarlyStopping(monitor = metric,\n",
    "                                               mode = \"min\",\n",
    "                                               start_from_epoch = early_stopping_warmup,\n",
    "                                               min_delta = early_stopping_min_delta,\n",
    "                                               patience = early_stopping_patience,\n",
    "                                               restore_best_weights = True)\n",
    "            self.callbacks.append(es)\n",
    "\n",
    "        # If batch_size is unspecified, set it to be the training size. Note that decreasing the batch size to smaller values, such as 500 for example, has previously lead the\n",
    "        # model to converge too early, leading to a lot of time of investigation. When dealing with neural networks in the statistical models context, we recommend to use a single\n",
    "        # batch in training. Alternatives in the case that the sample is too big might be to consider a \"gradient accumulation\" approach.\n",
    "        self.train_batch_size = train_batch_size\n",
    "        if(self.train_batch_size is None):\n",
    "            self.train_batch_size = self.x_train.shape[0]\n",
    "\n",
    "        self.val_batch_size = val_batch_size\n",
    "        if(self.val_batch_size is None):\n",
    "            self.val_batch_size = self.x_val.shape[0]\n",
    "        \n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        if(self.gradient_accumulation_steps is None):\n",
    "            # The number of batches until the actual weights update (we ensure that the weights are updated only once per epoch, even though we might have multiple batches)\n",
    "            self.gradient_accumulation_steps = int(np.ceil( self.x_train.shape[0] / self.train_batch_size ))\n",
    "\n",
    "        self.compile_model(optimizer_independent_pars = optimizer_independent_pars, optimizer_nn = optimizer_nn, run_eagerly = run_eagerly)\n",
    "\n",
    "        # Create the training dataset\n",
    "        self.buffer_size = buffer_size\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((self.x_train, self.t_train, self.delta_train))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size = self.buffer_size).batch(self.train_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.fit(\n",
    "            train_dataset,\n",
    "            epochs = epochs,\n",
    "            verbose = 0,\n",
    "            callbacks = self.callbacks,\n",
    "            batch_size = self.train_batch_size,\n",
    "            shuffle = shuffle\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "da042c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baec5ab692bb47f38853a698824f20ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FrailtyModelNN(None, None, input_dim = (None, 1), seed = 10)\n",
    "\n",
    "np.random.seed(10)\n",
    "x = np.random.normal(size = 10)\n",
    "t = np.random.exponential(size = 10)\n",
    "delta = np.random.binomial(n = 1, p = 0.5, size = 10)\n",
    "\n",
    "model.train_model(x, t, delta,\n",
    "                  epochs = 100, shuffle = True,\n",
    "                  optimizer_independent_pars = optimizers.Adam(learning_rate = 0.01),\n",
    "                  optimizer_nn = optimizers.Adam(learning_rate = 0.01),\n",
    "                  train_batch_size = None, val_batch_size = None,\n",
    "                  buffer_size = 4096, gradient_accumulation_steps = None,\n",
    "                  run_eagerly = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7a651eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.3184538,\n",
       " array([[ 0.05222978, -0.42660013,  0.15580629, -0.03946069,  0.4755298 ,\n",
       "          0.48080763,  0.4745159 , -0.23191053,  0.25770208,  0.45803365,\n",
       "          0.13718636,  0.53897285,  0.42468554,  0.10244699,  0.2735144 ,\n",
       "         -0.3586141 ]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([[ 4.59125452e-02, -3.75002444e-01,  1.36961371e-01,\n",
       "         -3.46878804e-02,  4.18014020e-01,  4.22653496e-01],\n",
       "        [ 4.17122751e-01, -2.03860730e-01,  2.26532787e-01,\n",
       "          4.02634084e-01,  1.20593548e-01,  4.73783612e-01],\n",
       "        [ 3.73319447e-01,  9.00559351e-02,  2.40432560e-01,\n",
       "         -3.15239400e-01,  4.11400080e-01, -4.99688447e-01],\n",
       "        [-1.54003322e-01,  3.98376286e-01, -6.77022561e-02,\n",
       "         -6.87515596e-03,  2.22874284e-02,  6.38603926e-01],\n",
       "        [-2.10074767e-01,  5.15375519e-03, -6.19977772e-01,\n",
       "          4.71546680e-01,  2.97671575e-02,  3.47635865e-01],\n",
       "        [-4.51327175e-01,  3.83570552e-01, -4.70652610e-01,\n",
       "         -8.76119826e-03, -4.18085754e-01, -9.70348269e-02],\n",
       "        [-2.10167021e-01,  2.06117809e-01, -4.87221852e-02,\n",
       "         -1.66043086e-04, -2.08353534e-01,  6.84473366e-02],\n",
       "        [ 9.24817175e-02, -3.49625111e-01, -4.11173493e-01,\n",
       "          2.26042792e-01,  3.02696712e-02,  4.96728122e-01],\n",
       "        [-1.04874596e-01, -5.87778151e-01, -3.05568397e-01,\n",
       "          2.46745963e-02, -3.33385527e-01, -3.22777152e-01],\n",
       "        [ 1.03437640e-01, -1.83662534e-01, -1.05925195e-01,\n",
       "         -3.96862686e-01, -2.95539469e-01,  2.44550496e-01],\n",
       "        [ 3.30307394e-01, -6.08116165e-02,  5.42210042e-01,\n",
       "          3.88013363e-01, -9.24480483e-02, -4.98313010e-01],\n",
       "        [-2.67496526e-01, -3.12220216e-01,  1.91735446e-01,\n",
       "          1.50903583e-01,  4.54178870e-01, -1.83140099e-01],\n",
       "        [-2.47560859e-01,  3.91313612e-01, -1.13742039e-01,\n",
       "          2.60600418e-01,  5.58439314e-01, -8.73253942e-02],\n",
       "        [ 5.18797219e-01,  3.09211433e-01, -1.45055711e-01,\n",
       "         -3.98328364e-01, -2.19573095e-01, -1.53550178e-01],\n",
       "        [-6.11535013e-01, -2.41578668e-01,  4.13956523e-01,\n",
       "         -2.46892795e-01, -6.85024858e-01,  2.60797203e-01],\n",
       "        [-1.31117776e-01,  2.91932613e-01, -9.35722515e-02,\n",
       "          1.33604661e-01,  8.50617066e-02,  3.88585955e-01]], dtype=float32)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fa089fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[0.4917291 ],\n",
       "       [0.71194553],\n",
       "       [1.4696095 ],\n",
       "       [1.0033522 ],\n",
       "       [0.7488525 ],\n",
       "       [1.2633488 ],\n",
       "       [0.8924959 ],\n",
       "       [0.9562993 ],\n",
       "       [0.99828273],\n",
       "       [1.0688634 ]], dtype=float32)>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.exp( tf.gather( model.predict( x[:,None] ), [0], axis = 1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef3356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3405b9c3ff945b580b5042a81604d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_theta: [[-0.00948396]\n",
      " [-0.0276411 ]\n",
      " [ 0.18126766]\n",
      " [ 0.09189229]\n",
      " [ 0.17733589]\n",
      " [-0.04042894]\n",
      " [ 0.00667084]\n",
      " [ 0.12130801]\n",
      " [ 0.37508252]\n",
      " [-0.03350516]]\n",
      "raw_tau: [[ 0.37486914  0.3583766  -0.00391461  0.11909363  0.413968  ]\n",
      " [ 0.05663535  0.01132016  0.04765547  0.05542642  0.06414704]\n",
      " [-0.12494157  0.17425172 -0.30946922 -0.29076833 -0.13145213]\n",
      " [-0.08244592  0.06923717 -0.15583271 -0.15276313 -0.08781268]\n",
      " [-0.12342028  0.1692943  -0.30262083 -0.28478065 -0.12984766]\n",
      " [ 0.13562647  0.06976443  0.0701836   0.0966004   0.15561514]\n",
      " [-0.00924998  0.00173164 -0.01137376 -0.01205722 -0.01022647]\n",
      " [-0.09860086  0.10167401 -0.20598343 -0.19873121 -0.10428607]\n",
      " [-0.1791718   0.436904   -0.66195965 -0.58252394 -0.20213403]\n",
      " [ 0.2389637   0.18477742  0.05500455  0.11640279  0.27284223]]\n",
      "alpha: 0.5\n",
      "gamma: 1.0\n",
      "phi1: 1.0\n",
      "phi2: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value ({'raw_alpha': <Variable path=frailty_model_nn_7/raw_alpha, shape=(), dtype=float32, value=0.0>, 'raw_gamma': <Variable path=frailty_model_nn_7/raw_gamma, shape=(), dtype=float32, value=0.0>, 'raw_phi1': <Variable path=frailty_model_nn_7/raw_phi1, shape=(), dtype=float32, value=0.0>, 'raw_phi2': <Variable path=frailty_model_nn_7/raw_phi2, shape=(), dtype=float32, value=0.0>}) with an unsupported type (<class 'keras.src.utils.tracking.TrackedDict'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexponential(size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      7\u001b[0m delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 271\u001b[0m, in \u001b[0;36mFrailtyModelNN.train_model\u001b[0;34m(self, x, t, delta, epochs, shuffle, optimizer, train_batch_size, val_batch_size, buffer_size, gradient_accumulation_steps, run_eagerly, verbose)\u001b[0m\n\u001b[1;32m    268\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_train))\n\u001b[1;32m    269\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(buffer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch_size)\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[19], line 188\u001b[0m, in \u001b[0;36mFrailtyModelNN.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    184\u001b[0m     nn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# AQUI TENTAMOS IMPLEMENTAR A IDEIA QUE TA EM UPPER CASE NA FUNÇÃO PERDA!!!\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     likelihood_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnn_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# print(\"Trainable variables:\\n\")\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# print(self.trainable_variables)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(likelihood_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[0;32mIn[19], line 174\u001b[0m, in \u001b[0;36mFrailtyModelNN.loss_func\u001b[0;34m(self, variables, nn_output, t, delta)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi2: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat( phi2 ))\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# log_f0 = tf.math.log(phi1) + (phi1-1)*tf.math.log(y) + phi2 - tf.math.exp(phi2) * tf.math.pow(y, phi1)\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# F0 = 1 - tf.math.exp(-tf.math.pow(y, phi1) * tf.math.exp(phi2))\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# laplace_transform_term = 1 + gamma*theta*F0/(1-alpha)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# return loss_weights_mean\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mconstant( \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m )\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value ({'raw_alpha': <Variable path=frailty_model_nn_7/raw_alpha, shape=(), dtype=float32, value=0.0>, 'raw_gamma': <Variable path=frailty_model_nn_7/raw_gamma, shape=(), dtype=float32, value=0.0>, 'raw_phi1': <Variable path=frailty_model_nn_7/raw_phi1, shape=(), dtype=float32, value=0.0>, 'raw_phi2': <Variable path=frailty_model_nn_7/raw_phi2, shape=(), dtype=float32, value=0.0>}) with an unsupported type (<class 'keras.src.utils.tracking.TrackedDict'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "model = FrailtyModelNN(None, None)\n",
    "\n",
    "model.define_structure()\n",
    "\n",
    "x = np.random.normal(size = 10)\n",
    "t = np.random.exponential(size = 10)\n",
    "delta = np.random.binomial(n = 1, p = 0.5, size = 10)\n",
    "\n",
    "model.train_model(x, t, delta,\n",
    "                  epochs = 10, shuffle = True,\n",
    "                  optimizer = optimizers.Adam(learning_rate = 0.001),\n",
    "                  train_batch_size = None, val_batch_size = None,\n",
    "                  buffer_size = 4096, gradient_accumulation_steps = None,\n",
    "                  run_eagerly = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "952922e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw_alpha'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables[0].path.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1de8430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.7739912 ],\n",
       "       [-0.86790764],\n",
       "       [-0.7947914 ],\n",
       "       [ 0.41015804],\n",
       "       [ 0.97464114],\n",
       "       [ 0.04176952],\n",
       "       [ 1.498434  ],\n",
       "       [-0.72059375],\n",
       "       [-0.85633576],\n",
       "       [ 0.35985187]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x[:,None])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyenv 3.10.17)",
   "language": "python",
   "name": "pyenv-3.10.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
