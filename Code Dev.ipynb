{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7b393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 03:50:23.089351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760424623.107637  244814 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760424623.113214  244814 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760424623.127713  244814 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760424623.127738  244814 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760424623.127741  244814 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760424623.127744  244814 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-14 03:50:23.132532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1760424627.165740  244814 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4336 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "from keras import layers, initializers, optimizers, losses\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22172881",
   "metadata": {},
   "source": [
    "In order for we to not have to import tensorflow every time we want to test something. Development for the models will be made here and then migrated to the proper .py files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85583418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrailtyModelNN(keras.models.Model):\n",
    "\n",
    "    def __init__(self, parameters, loglikelihood_loss, neural_network_structure = None, neural_network_call = None, input_dim = None, seed = None):\n",
    "        super().__init__()\n",
    "        self.parameters = parameters\n",
    "        self.loglikelihood_loss = loglikelihood_loss\n",
    "        self.neural_network_structure = neural_network_structure\n",
    "        self.neural_network_call = neural_network_call\n",
    "        self.n_acum_step = tf.Variable(0, dtype = tf.int32, trainable = False)\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "        if(input_dim is not None):\n",
    "            self.define_structure(input_dim)\n",
    "    \n",
    "    def define_structure(self, input_dim):\n",
    "        # Goes through the list of parameters for the model and filter them by their classes:\n",
    "        # - \"nn\" will be treated as an output from a given neural network that receives the variables x as input.\n",
    "        # - \"independet\" will be treated an an individual tf.Variable, trainable object. It is still trained in tensorflow, but is constant for all subjects\n",
    "        # - \"fixed\" will be treated as a non-trainable tf.Variable. Basically just a known constant.\n",
    "        # - \"manual\" will be treated as a non-trainable tf.Variable, but its value will be eventually updated manually using user provided functions (useful in cases where closed forms can be obtained)\n",
    "        # - \"dependent\" will be treated simply as a deterministic function of other parameters and will be updated after training\n",
    "\n",
    "        self.nn_pars = []\n",
    "        self.independent_pars = []\n",
    "        self.fixed_pars = []\n",
    "        self.manual_pars = []\n",
    "        for parameter in self.parameters:\n",
    "            par = self.parameters[parameter]\n",
    "            if(par[\"par_type\"] == \"nn\"):\n",
    "                self.nn_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"independent\"):\n",
    "                self.independent_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"fixed\"):\n",
    "                self.fixed_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"manual\"):\n",
    "                self.manual_pars.append( parameter )\n",
    "            else:\n",
    "                raise Exception(\"Invalid parameter {} type: {}\".format(parameter, par[\"par_type\"]))\n",
    "\n",
    "        # If at least one parameter is to be modeled as a neural network output, define its architecture here\n",
    "        if( len(self.nn_pars) > 0 ):\n",
    "            if(self.neural_network_structure is None):\n",
    "                raise Exception(\"Parameters {} defined as 'nn'. Please, provide a structure for their neural network.\".format(self.nn_pars))\n",
    "            # Define the neural network structure based on the user's input\n",
    "            self.neural_network_structure(self, self.seed)\n",
    "\n",
    "        # Dictionary with all parameters that are its individual weights\n",
    "        self.model_variables = {}\n",
    "\n",
    "        # Include variables that do not depend on the variables x, but are still trained by tensorflow\n",
    "        for parameter in self.independent_pars:\n",
    "            par = self.parameters[parameter]\n",
    "\n",
    "            # If shape is None, set it to ()\n",
    "            if(par[\"shape\"] is None):\n",
    "                par[\"shape\"] = ()\n",
    "\n",
    "            raw_init = par[\"link_inv\"]( par[\"init\"] )\n",
    "\n",
    "            # Name for the new, transformed parameter\n",
    "            raw_parameter = \"raw_\" + parameter\n",
    "            self.model_variables[raw_parameter] = self.add_weight(\n",
    "                name = raw_parameter,\n",
    "                shape = par[\"shape\"],\n",
    "                initializer = keras.initializers.Constant( raw_init ),\n",
    "                trainable = True,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "\n",
    "        # Include variables that are not trained by tensorflow (known, fixed constants or manual trained variables)\n",
    "        for parameter in np.concatenate([self.fixed_pars, self.manual_pars]):\n",
    "            par = self.parameters[parameter]\n",
    "            self.model_variables[parameter] = self.add_weight(\n",
    "                name = parameter,\n",
    "                shape = par[\"shape\"],\n",
    "                initializer = keras.initializers.Constant( par[\"initializer\"] ),\n",
    "                trainable = False,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "\n",
    "        # Organize trainable variables information, so each variable can get mapped to an index in the self.trainable_variables and its gradients\n",
    "        self.vars_to_index = {}\n",
    "        # Before we build the model, the only variables that appear in here are the ones corresponding to \"independent\" parameters\n",
    "        for i, var in enumerate(self.trainable_variables):\n",
    "            # From the variable path, get its name (raw_<variable>)\n",
    "            var_name = var.path.split(\"/\")[-1]\n",
    "            # Save its corresponding index\n",
    "            self.vars_to_index[var_name] = i\n",
    "\n",
    "        nn_par_index = 0\n",
    "        # We must also include in this list the indices for \"nn\" parameters\n",
    "        for i, parameter in enumerate(self.nn_pars):\n",
    "            par = self.parameters[ parameter ]\n",
    "            if(par[\"shape\"] is None):\n",
    "                par_shape = 1\n",
    "            else:\n",
    "                # The parameter must be at most a 1-dimensional array, whose indices will be saved for future location in the neural network output results\n",
    "                par_shape = par[\"shape\"]\n",
    "            \n",
    "            self.vars_to_index[\"raw_\" + parameter] = tf.constant( np.arange(nn_par_index, nn_par_index+par_shape), dtype = tf.int32 )\n",
    "            nn_par_index += par_shape\n",
    "\n",
    "\n",
    "        # Once the entire structure has been defined, force the model to build all the weights properly\n",
    "        dummy_input = keras.Input(input_dim)\n",
    "        self(dummy_input)\n",
    "\n",
    "\n",
    "        # In the future, it might be interesting to allow the user to specify an optimizer for each single parameter in the model.\n",
    "        # For now, they will specify one for the independent parameters and other for the neural network weights\n",
    "\n",
    "        # Not that the model is built and all the trainable variables instantiated, we define the gradient variables\n",
    "        self.gradient_accumulation_independent_pars = [\n",
    "            tf.Variable(tf.zeros_like(v, dtype = tf.float32), trainable = False) for v in self.trainable_variables[ :len(self.independent_pars) ]\n",
    "        ]\n",
    "\n",
    "        # The gradient values for the neural network component always comes right after the weights for the independent parameters\n",
    "        self.gradient_accumulation_nn = [\n",
    "            tf.Variable(tf.zeros_like(v, dtype = tf.float32), trainable = False) for v in self.trainable_variables[ len(self.independent_pars): ]\n",
    "        ]\n",
    "\n",
    "\n",
    "    def call(self, x_input):\n",
    "        if(self.neural_network_call is None):\n",
    "            return None\n",
    "        return self.neural_network_call(self, x_input)\n",
    "\n",
    "    def get_variable(self, parameter, nn_output = None):\n",
    "        \"\"\"\n",
    "            Once that all variables have been properly defined and mapped, this method uses their proper link functions to transform from\n",
    "            the variables 'raw' state into their proper values used in the likelihood.\n",
    "\n",
    "            If nn_output is passed, we automatically assume that the parameter is an output from the neural network and proceed by taking its\n",
    "            value differently than if it was an independent parameter.\n",
    "        \"\"\"\n",
    "        # Get the raw name for that parameter\n",
    "        raw_parameter = \"raw_\" + parameter\n",
    "        # Filter the desired parameter from the list\n",
    "        par = self.parameters[parameter]\n",
    "\n",
    "        # If nn_output is None, assume the parameter is independent from the data x and get it directly as a transformed weight\n",
    "        if(nn_output is None):            \n",
    "            # Get the transformed parameter from its raw version, considering its proper link function\n",
    "            return par[\"link\"]( self.model_variables[raw_parameter] )\n",
    "        \n",
    "        # If nn_output is not None, assume the parameter came as a neural network output and return it from its positions in the output\n",
    "        return par[\"link\"]( tf.gather(nn_output, self.vars_to_index[raw_parameter], axis = 1) )\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "            Called by each batch in order to evaluate the loglikelihood and accumulate the parameters gradients using training data.\n",
    "        \"\"\"\n",
    "        x, t, delta = data\n",
    "\n",
    "        self.n_acum_step.assign_add(1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            nn_output = self(x, training = True)\n",
    "            # likelihood_loss = self.loss_func(nn_output = nn_output, t = t, delta = delta)\n",
    "            loss_value = self.loglikelihood_loss(model = self, nn_output = nn_output, t = t, delta = delta)\n",
    "\n",
    "        # The first weights are always destined to the fixed parameters\n",
    "        # The neural network related weights comes after those in the self.trainable_variables object\n",
    "        gradients = tape.gradient(loss_value, self.trainable_variables)\n",
    "\n",
    "        # If the loss does not depend on a specific parameter, its corresponding gradient will be None\n",
    "        # To avoid crash problems in that case, we simply replace None with a zero like gradient, so those weights do not get updated\n",
    "        # It is the user's responsibility to build a loss that depends on all the trainable parameters, but we allow that to happen in this case\n",
    "        # for generality and to avoid unneccessary crashes when testing new models\n",
    "        gradients = [\n",
    "            g if g is not None else tf.zeros_like(v)\n",
    "            for g, v in zip(gradients, self.trainable_variables)\n",
    "        ]\n",
    "\n",
    "        independent_gradients = gradients[ :len(self.independent_pars) ]\n",
    "        nn_gradients = gradients[ len(self.independent_pars): ]\n",
    "\n",
    "        for i in range( len(self.gradient_accumulation_independent_pars) ):\n",
    "            self.gradient_accumulation_independent_pars[i].assign_add( independent_gradients[i] )\n",
    "\n",
    "        for i in range( len(self.gradient_accumulation_nn) ):\n",
    "            self.gradient_accumulation_nn[i].assign_add( nn_gradients[i] )\n",
    "\n",
    "        tf.cond(tf.equal(self.n_acum_step, self.gradient_accumulation_steps), self.apply_accumulated_gradients, lambda: None)\n",
    "\n",
    "        return {\"likelihood_loss\": loss_value}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, t, delta = data\n",
    "        nn_output = self(x, training = False)\n",
    "        likelihood_loss = self.loss_func(nn_output = nn_output, t = t, delta = delta)\n",
    "        return {\"likelihood_loss\": likelihood_loss}\n",
    "\n",
    "    def apply_accumulated_gradients(self):\n",
    "        # ----------------------------------- Independent parameters component -----------------------------------\n",
    "        # Apply the accumulated gradients to the trainable variables\n",
    "        self.optimizer_independent_pars.apply_gradients( zip(self.gradient_accumulation_independent_pars, self.trainable_variables[ :len(self.independent_pars) ]) )\n",
    "        # Resets all the cumulated gradients to zero\n",
    "        for i in range(len(self.gradient_accumulation_nn)):\n",
    "            self.gradient_accumulation_independent_pars[i].assign(tf.zeros_like(self.trainable_variables[ :len(self.independent_pars) ][i], dtype = tf.float32))\n",
    "        \n",
    "        # ----------------------------------- Neural network component -----------------------------------\n",
    "        self.optimizer_nn.apply_gradients( zip(self.gradient_accumulation_nn, self.trainable_variables[ len(self.independent_pars): ]) )\n",
    "        # Resets all the cumulated gradients to zero\n",
    "        for i in range(len(self.gradient_accumulation_nn)):\n",
    "            self.gradient_accumulation_nn[i].assign(tf.zeros_like(self.trainable_variables[ len(self.independent_pars): ][i], dtype = tf.float32))\n",
    "\n",
    "        # Reset the gradient accumulation steps counter to zero\n",
    "        self.n_acum_step.assign(0)\n",
    "\n",
    "    def compile_model(self, optimizer_independent_pars, optimizer_nn, run_eagerly):\n",
    "        \"\"\"\n",
    "            Defines the configuration for the model, such as batch size, training mode, early stopping.\n",
    "        \"\"\"\n",
    "        # optimizers.Adam(learning_rate = learning_rate, gradient_accumulation_steps = None),\n",
    "        self.optimizer_independent_pars = optimizer_independent_pars\n",
    "        self.optimizer_nn = optimizer_nn\n",
    "        self.compile(\n",
    "            run_eagerly = run_eagerly\n",
    "        )\n",
    "\n",
    "    def train_model(self, x, t, delta,\n",
    "                    epochs, shuffle,\n",
    "                    validation = False, val_prop = None, x_val = None, t_val = None, delta_val = None,\n",
    "                    optimizer_independent_pars = optimizers.Adam(learning_rate = 0.001),\n",
    "                    optimizer_nn = optimizers.Adam(learning_rate = 0.001),\n",
    "                    train_batch_size = None, val_batch_size = None,\n",
    "                    buffer_size = 4096, gradient_accumulation_steps = None,\n",
    "                    early_stopping = True, early_stopping_min_delta = 0.0, early_stopping_patience = 10, early_stopping_warmup = 0,\n",
    "                    run_eagerly = True, verbose = 1):\n",
    "        \"\"\"\n",
    "            This is the function that start the training.\n",
    "        \"\"\"\n",
    "        self.validation = validation\n",
    "\n",
    "        # Pass the input variables to tensorflow default types\n",
    "        x = tf.cast(x, dtype = tf.float32)\n",
    "        t = tf.cast(t, dtype = tf.float32)\n",
    "        delta = tf.cast(delta, dtype = tf.float32)\n",
    "        \n",
    "        # If input is a vector, transform it into a column\n",
    "        if(len(x.shape) == 1):\n",
    "            x = tf.reshape( x, shape = (len(x), 1) )\n",
    "        if(len(t.shape) == 1):\n",
    "            t = tf.reshape( t, shape = (len(t), 1) )\n",
    "        if(len(delta.shape) == 1):\n",
    "            delta = tf.reshape( delta, shape = (len(delta), 1) )\n",
    "\n",
    "        # Salva os dados originais\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.delta = delta\n",
    "\n",
    "        if(self.validation):\n",
    "            # If all validation data was given\n",
    "            if(x_val is not None and t_val is not None and delta_val is not None):\n",
    "                x_val = tf.cast(x_val, dtype = tf.float32)\n",
    "                t_val = tf.cast(t_val, dtype = tf.float32)\n",
    "                delta_val = tf.cast(delta_val, dtype = tf.float32)\n",
    "\n",
    "                if(len(x_val.shape) == 1):\n",
    "                    x_val = tf.reshape( x_val, shape = (len(x_val), 1) )\n",
    "                if(len(t_val.shape) == 1):\n",
    "                    t_val = tf.reshape( t_val, shape = (len(t_val), 1) )\n",
    "                if(len(delta_val.shape) == 1):\n",
    "                    delta_val = tf.reshape( delta_val, shape = (len(delta_val), 1) )\n",
    "                \n",
    "                self.x_val = x_val\n",
    "                self.t_val = t_val\n",
    "                self.delta_val = delta_val\n",
    "                self.x_train, self.t_train, self.delta_train = self.x, self.t, self.delta\n",
    "            # If validation is desired, but no data was given, select val_prop * 100% observations as validation set\n",
    "            else:\n",
    "                self.indexes_train = np.arange(x.shape[0])\n",
    "                if(shuffle):\n",
    "                    self.indexes_train = tf.random.shuffle( self.indexes_train )\n",
    "                    \n",
    "                x_shuffled = tf.gather( x, self.indexes_train )\n",
    "                t_shuffled = tf.gather( t, self.indexes_train )\n",
    "                delta_shuffled = tf.gather( delta, self.indexes_train )\n",
    "\n",
    "                if(val_prop is None):\n",
    "                    raise Exception(\"Please, provide the size of the validation set (between 0 and 1).\")\n",
    "                # Selects the subsample as validation data\n",
    "                val_size = int(x.shape[0] * val_prop)\n",
    "                self.x_val, self.t_val, self.delta_val = x_shuffled[:val_size], t_shuffled[:val_size], delta_shuffled[:val_size]\n",
    "                self.x_train, self.t_train, self.delta_train = x_shuffled[val_size:], t_shuffled[val_size:], delta_shuffled[val_size:]\n",
    "        else:\n",
    "            # If no validation step should be taken, training data is the same as validation data\n",
    "            self.x_train, self.t_train, self.delta_train = self.x, self.t, self.delta\n",
    "            self.x_val, self.t_val, self.delta_val = self.x, self.t, self.delta\n",
    "        \n",
    "        # Declara os callbacks do modelo\n",
    "        self.callbacks = [ ]\n",
    "        \n",
    "        if(verbose >= 1):\n",
    "            self.callbacks.append( TqdmCallback(verbose = 0, position = 0, leave = True) )\n",
    "        \n",
    "        if(early_stopping):\n",
    "            # Avoids overfitting and speeds training\n",
    "            if(self.validation):\n",
    "                metric = \"val_likelihood_loss\"\n",
    "            else:\n",
    "                metric = \"likelihood_loss\"\n",
    "            es = keras.callbacks.EarlyStopping(monitor = metric,\n",
    "                                               mode = \"min\",\n",
    "                                               start_from_epoch = early_stopping_warmup,\n",
    "                                               min_delta = early_stopping_min_delta,\n",
    "                                               patience = early_stopping_patience,\n",
    "                                               restore_best_weights = True)\n",
    "            self.callbacks.append(es)\n",
    "\n",
    "        # If batch_size is unspecified, set it to be the training size. Note that decreasing the batch size to smaller values, such as 500 for example, has previously lead the\n",
    "        # model to converge too early, leading to a lot of time of investigation. When dealing with neural networks in the statistical models context, we recommend to use a single\n",
    "        # batch in training. Alternatives in the case that the sample is too big might be to consider a \"gradient accumulation\" approach.\n",
    "        self.train_batch_size = train_batch_size\n",
    "        if(self.train_batch_size is None):\n",
    "            self.train_batch_size = self.x_train.shape[0]\n",
    "\n",
    "        self.val_batch_size = val_batch_size\n",
    "        if(self.val_batch_size is None):\n",
    "            self.val_batch_size = self.x_val.shape[0]\n",
    "        \n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        if(self.gradient_accumulation_steps is None):\n",
    "            # The number of batches until the actual weights update (we ensure that the weights are updated only once per epoch, even though we might have multiple batches)\n",
    "            self.gradient_accumulation_steps = int(np.ceil( self.x_train.shape[0] / self.train_batch_size ))\n",
    "\n",
    "        self.compile_model(optimizer_independent_pars = optimizer_independent_pars, optimizer_nn = optimizer_nn, run_eagerly = run_eagerly)\n",
    "\n",
    "        # Create the training dataset\n",
    "        self.buffer_size = buffer_size\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((self.x_train, self.t_train, self.delta_train))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size = self.buffer_size).batch(self.train_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        val_dataset = None\n",
    "        if(validation):\n",
    "            # Create the validation dataset\n",
    "            val_dataset = tf.data.Dataset.from_tensor_slices((self.x_val, self.t_val, self.delta_val))\n",
    "            val_dataset = val_dataset.batch(self.val_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        self.fit(\n",
    "            train_dataset,\n",
    "            validation_data = val_dataset,\n",
    "            epochs = epochs,\n",
    "            verbose = 0,\n",
    "            callbacks = self.callbacks,\n",
    "            batch_size = self.train_batch_size,\n",
    "            shuffle = shuffle\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27ee3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood_loss_func(model, nn_output, t, delta):\n",
    "        \"\"\"\n",
    "            This is an example of a correctly defined loss function.\n",
    "        \"\"\"\n",
    "        theta = model.get_variable(\"theta\", nn_output)\n",
    "\n",
    "        alpha = model.get_variable(\"alpha\")\n",
    "        gamma = model.get_variable(\"gamma\")\n",
    "        phi1 = model.get_variable(\"phi1\")\n",
    "        phi2 = model.get_variable(\"phi2\")\n",
    "\n",
    "        log_f0 = tf.math.log(phi1) + (phi1-1)*tf.math.log(t) + phi2 - tf.math.exp(phi2) * tf.math.pow(t, phi1)\n",
    "        F0 = 1 - tf.math.exp(-tf.math.pow(t, phi1) * tf.math.exp(phi2))\n",
    "        laplace_transform_term = 1 + gamma*theta*F0/(1-alpha)\n",
    "\n",
    "        loss_weights = delta*tf.math.log(theta) + delta*log_f0 + (1-alpha)/(alpha*gamma)*(1 - tf.math.pow(laplace_transform_term, alpha)) + (alpha-1)*delta*tf.math.log(laplace_transform_term)\n",
    "        loss_weights_mean = -tf.math.reduce_mean(loss_weights)\n",
    "        \n",
    "        return loss_weights_mean\n",
    "\n",
    "def neural_network_structure(model, seed = None):\n",
    "    \"\"\" \n",
    "        This is an example of a correctly defined neural network structure definition function.\n",
    "    \"\"\"\n",
    "    initializer = initializers.GlorotNormal(seed = seed)\n",
    "    model.dense1 = keras.layers.Dense(units = 16, activation = \"gelu\", kernel_initializer = initializer, dtype = tf.float32, name = \"dense1\")\n",
    "    model.dense2 = keras.layers.Dense(units = 1, kernel_initializer = initializer, dtype = tf.float32, activation = None, use_bias = False, name = \"output\")\n",
    "\n",
    "def neural_network_call(model, x_input):\n",
    "    \"\"\"\n",
    "        This is an example of a correctly defined neural netowork call function. Its elements such as dense1 and dense2 must have been defined in neural_network_structure.\n",
    "        Otherwise, it will result in an error\n",
    "    \"\"\"\n",
    "    x = model.dense1(x_input)\n",
    "    return model.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da042c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f237263e8c64685b818cfca8f04bb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"theta\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"nn\", \"update_func\": None, \"shape\": None, \"initializer\": None},\n",
    "    \"alpha\": {\"link\": lambda x : 1/(1+tf.math.exp(-x)), \"link_inv\": lambda x : tf.math.log(x) - tf.math.log(1-x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 0.5},\n",
    "    \"gamma\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 1.0},\n",
    "    \"phi1\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 1.0},\n",
    "    \"phi2\": {\"link\": lambda x : tf.identity(x), \"link_inv\": lambda x : tf.identity(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 0.0}\n",
    "}\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    model = FrailtyModelNN(parameters, loglikelihood_loss_func, neural_network_structure, neural_network_call, input_dim = (None, 1), seed = 10)\n",
    "\n",
    "    np.random.seed(10)\n",
    "    x = np.random.normal(size = 10)\n",
    "    t = np.random.exponential(size = 10)\n",
    "    delta = np.random.binomial(n = 1, p = 0.5, size = 10)\n",
    "\n",
    "    model.train_model(x, t, delta,\n",
    "                        epochs = 2000, shuffle = True,\n",
    "                        validation = False, val_prop = 0.2,\n",
    "                        optimizer_independent_pars = optimizers.Adam(learning_rate = 0.005),\n",
    "                        optimizer_nn = optimizers.Adam(learning_rate = 0.0001),\n",
    "                        train_batch_size = None, val_batch_size = None,\n",
    "                        buffer_size = 4096, gradient_accumulation_steps = None,\n",
    "                        early_stopping = True, early_stopping_min_delta = 0.0, early_stopping_patience = 10, early_stopping_warmup = 0,\n",
    "                        run_eagerly = False, verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyenv 3.10.17)",
   "language": "python",
   "name": "pyenv-3.10.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
