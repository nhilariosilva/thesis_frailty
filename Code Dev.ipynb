{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7b393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 17:01:29.976363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760126489.994692   21381 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760126489.999808   21381 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760126490.014721   21381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760126490.014747   21381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760126490.014750   21381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760126490.014753   21381 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-10 17:01:30.019460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "from keras import layers, initializers, optimizers, losses\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22172881",
   "metadata": {},
   "source": [
    "In order for we to not have to import tensorflow every time we want to test something. Development for the models will be made here and then migrated to the proper .py files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85583418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrailtyModelNN(keras.models.Model):\n",
    "\n",
    "    def __init__(self, parameters, loglikelihood):\n",
    "        super().__init__()\n",
    "        self.parameters = {\n",
    "            \"theta\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"nn\", \"update_func\": None, \"shape\": None, \"initializer\": None},\n",
    "            \"tau\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"nn\", \"update_func\": None, \"shape\": 5, \"initializer\": None},\n",
    "            \"alpha\": {\"link\": lambda x : 1/(1+tf.math.exp(-x)), \"link_inv\": lambda x : tf.math.log(x) - tf.math.log(1-x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 0.5},\n",
    "            \"gamma\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 1.0},\n",
    "            \"phi1\": {\"link\": lambda x : tf.math.exp(x), \"link_inv\": lambda x : tf.math.log(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 1.0},\n",
    "            \"phi2\": {\"link\": lambda x : tf.identity(x), \"link_inv\": lambda x : tf.identity(x), \"par_type\": \"independent\", \"update_func\": None, \"shape\": None, \"init\": 0.0}\n",
    "        }\n",
    "        self.n_acum_step = tf.Variable(0, dtype = tf.int32, trainable = False)\n",
    "\n",
    "    def clone_architecture(self, model):\n",
    "        pass\n",
    "    \n",
    "    def define_structure(self):\n",
    "\n",
    "        # Goes through the list of parameters for the model and filter them by their classes:\n",
    "        # - \"nn\" will be treated as an output from a given neural network that receives the variables x as input.\n",
    "        # - \"independet\" will be treated an an individual tf.Variable, trainable object. It is still trained in tensorflow, but is constant for all subjects\n",
    "        # - \"fixed\" will be treated as a non-trainable tf.Variable. Basically just a known constant.\n",
    "        # - \"manual\" will be treated as a non-trainable tf.Variable, but its value will be eventually updated manually using user provided functions (useful in cases where closed forms can be obtained)\n",
    "        # - \"dependent\" will be treated simply as a deterministic function of other parameters and will be updated after training\n",
    "\n",
    "        nn_pars = []\n",
    "        independent_pars = []\n",
    "        fixed_pars = []\n",
    "        manual_pars = []\n",
    "        for parameter in self.parameters:\n",
    "            par = self.parameters[parameter]\n",
    "            if(par[\"par_type\"] == \"nn\"):\n",
    "                nn_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"independent\"):\n",
    "                independent_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"fixed\"):\n",
    "                fixed_pars.append( parameter )\n",
    "            elif(par[\"par_type\"] == \"manual\"):\n",
    "                manual_pars.append( parameter )\n",
    "            else:\n",
    "                raise Exception(\"Invalid parameter {} type: {}\".format(parameter, par[\"par_type\"]))\n",
    "\n",
    "        # If at least one parameter is to be modeled as a neural network output, define its architecture here\n",
    "        if( len(nn_pars) > 0 ):\n",
    "            # Try to implement a function that, given a model, clone its architecture\n",
    "            # clone_architecture(given_model)\n",
    "\n",
    "            # The user provided architecture must output an array with the exact same shape as the concatenation of all \"nn\" parameters.\n",
    "            # For example, if theta (nn) is a single constant and alpha (nn) is an array with 3 values, the expected output for the neural network\n",
    "            # component is 4. For that, these parameters must be at most one dimensional arrays. (No matrix, or more complex structured parameters!)\n",
    "            initializer = initializers.GlorotNormal(seed = 1)\n",
    "            self.dense1 = keras.layers.Dense(units = 16, activation = \"gelu\", kernel_initializer = initializer, dtype = tf.float32, name = \"dense1\")\n",
    "            self.dense2 = keras.layers.Dense(units = 6, kernel_initializer = initializer, dtype = tf.float32, activation = None, use_bias = False, name = \"output\")\n",
    "\n",
    "        # Dictionary with all parameters that are its individual weights\n",
    "        self.model_variables = {}\n",
    "\n",
    "        # Include variables that do not depend on the variables x, but are still trained by tensorflow\n",
    "        for parameter in independent_pars:\n",
    "            par = self.parameters[parameter]\n",
    "\n",
    "            # If shape is None, set it to ()\n",
    "            if(par[\"shape\"] is None):\n",
    "                par[\"shape\"] = ()\n",
    "\n",
    "            raw_init = par[\"link_inv\"]( par[\"init\"] )\n",
    "\n",
    "            self.model_variables[parameter] = self.add_weight(\n",
    "                name = \"raw_\" + parameter,\n",
    "                shape = par[\"shape\"],\n",
    "                initializer = keras.initializers.Constant( raw_init ),\n",
    "                trainable = True,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "\n",
    "        # Include variables that are not trained by tensorflow (known, fixed constants or manual trained variables)\n",
    "        for parameter in np.concatenate([fixed_pars, manual_pars]):\n",
    "            par = self.parameters[parameter]\n",
    "            self.model_variables[parameter] = self.add_weight(\n",
    "                name = parameter,\n",
    "                shape = par[\"shape\"],\n",
    "                initializer = keras.initializers.Constant( par[\"initializer\"] ),\n",
    "                trainable = False,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "\n",
    "        # print(\"init trainable variables\\n\")\n",
    "        # print(self.trainable_variables)\n",
    "\n",
    "        # Organize trainable variables information, so each variable can get mapped to an index in the self.trainable_variables and its gradients\n",
    "        self.vars_to_index = {}\n",
    "        # Before we build the model, the only variables that appear in here are the ones corresponding to \"independent\" parameters\n",
    "        for i, var in enumerate(self.trainable_variables):\n",
    "            # From the variable path, get its name (raw_<variable>)\n",
    "            var_name = var.path.split(\"/\")[-1]\n",
    "            # Save its corresponding index\n",
    "            self.vars_to_index[var_name] = i\n",
    "\n",
    "        print(\"INCLUDING NN PARAMETERS IN THE LIST...\")\n",
    "        nn_par_index = 0\n",
    "        # We must also include in this list the indices for \"nn\" parameters\n",
    "        for i, parameter in enumerate(nn_pars):\n",
    "            par = self.parameters[ parameter ]\n",
    "            if(par[\"shape\"] is None):\n",
    "                par_shape = 1\n",
    "            else:\n",
    "                # The parameter must be at most a 1-dimensional array, whose indices will be saved for future location in the neural network output results\n",
    "                par_shape = par[\"shape\"]\n",
    "            \n",
    "            self.vars_to_index[\"raw_\" + parameter] = tf.constant( np.arange(nn_par_index, nn_par_index+par_shape), dtype = tf.int32 )\n",
    "            nn_par_index += par_shape\n",
    "        \n",
    "        print(\"VARIABLES MAPPING TO INDEX\")\n",
    "        print(self.vars_to_index)\n",
    "\n",
    "    def call(self, x_input):\n",
    "        x = self.dense1(x_input)\n",
    "        return self.dense2(x)\n",
    "\n",
    "    def loss_func(self, variables, nn_output, t, delta):\n",
    "        \"\"\"\n",
    "            This is an example of a correctly defined loss function.\n",
    "\n",
    "            - eta: Corresponds to the output of the neural network with input x - Corresponds to the \n",
    "        \"\"\"\n",
    "        \n",
    "        # ACREDITO QUE DE PRA FAZER ESSA AUTOMAÇÃO DE FORMA AUTOMATIZADA, JÁ QUE O USUÁRIO FORNECE A LINK FUNCTION NO INÍCIO\n",
    "        # UMA IDEIA É APENAS FORÇAR COM QUE loss_func CUSTOMIZADA SEJA INSERIDA ACEITANDO UM PARAMETRO PARAMETERS\n",
    "        # EM QUE PAREMETERS SERÁ SIMPLESMENTE UM DICIONÁRIO QUE JÁ COSPE OS VALORES DOS PARÂMETROS PRONTINHO\n",
    "        # MAS DIFICIL, JÁ QUE COMO O TENSORFLOW DEVE PRESERVAR A ESTRUTURA DOS SEUS OBJETOS PARA CALCULAR AS DERIVADAS PODE NÃO SER POSSIVEL SEGMENTAR ANTES\n",
    "        # TALVEZ VALHA A PENA DEIXAR COMO ESTÁ. FICA MAIS DIFÍCIL DE ENTENDER, MAS MANTEM A FLEXIBILIDADE PARA O USUÁRIO\n",
    "        raw_theta = tf.gather(nn_output, self.vars_to_index[\"raw_theta\"], axis = 1)\n",
    "        raw_tau = tf.gather(nn_output, self.vars_to_index[\"raw_tau\"], axis = 1)\n",
    "        print(variables)\n",
    "        alpha = 1/(1+tf.math.exp(-variables[\"raw_alpha\"]))\n",
    "        gamma = tf.math.exp( variables[\"raw_gamma\"] )\n",
    "        phi1 = tf.math.exp( variables[\"raw_phi1\"] )\n",
    "        phi2 = tf.identity( variables[\"raw_phi2\"] )\n",
    "\n",
    "        # log_f0 = tf.math.log(phi1) + (phi1-1)*tf.math.log(y) + phi2 - tf.math.exp(phi2) * tf.math.pow(y, phi1)\n",
    "        # F0 = 1 - tf.math.exp(-tf.math.pow(y, phi1) * tf.math.exp(phi2))\n",
    "        # laplace_transform_term = 1 + gamma*theta*F0/(1-alpha)\n",
    "\n",
    "        # loss_weights = delta*eta + delta*log_f0 + (1-alpha)/(alpha*gamma)*(1 - tf.math.pow(laplace_transform_term, alpha)) + (alpha-1)*delta*tf.math.log(laplace_transform_term)\n",
    "        # loss_weights_mean = -tf.math.reduce_mean(loss_weights)\n",
    "        \n",
    "        # return loss_weights_mean\n",
    "\n",
    "        return tf.constant( tf.math.reduce_mean(variables) )\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\"\n",
    "            Called by each batch in order to evaluate the loglikelihood and accumulate the parameters gradients using training data.\n",
    "        \"\"\"\n",
    "        x, t, delta = data\n",
    "\n",
    "        self.n_acum_step.assign_add(1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            nn_output = self(x, training = True)\n",
    "\n",
    "            # likelihood_loss = self.loss_func(variables = self.model_variables, nn_output = nn_output, t = t, delta = delta)\n",
    "\n",
    "        # print(\"Trainable variables:\\n\")\n",
    "        # print(self.trainable_variables)\n",
    "\n",
    "        gradients = tape.gradient(likelihood_loss, self.trainable_variables)\n",
    "\n",
    "        # print(\"GRADIENTS:\\n\")\n",
    "        # print(gradients)\n",
    "\n",
    "        return {\"likelihood_loss\": likelihood_loss}\n",
    "\n",
    "    def compile_model(self, optimizer, learning_rate, run_eagerly):\n",
    "        \"\"\"\n",
    "            Defines the configuration for the model, such as batch size, training mode, early stopping.\n",
    "        \"\"\"\n",
    "        # optimizer = optimizers.Adam(learning_rate = learning_rate, gradient_accumulation_steps = None),\n",
    "        self.optimizer = optimizer\n",
    "        self.compile(\n",
    "            run_eagerly = run_eagerly\n",
    "        )\n",
    "\n",
    "    def train_model(self, x, t, delta,\n",
    "                    epochs, shuffle = True,\n",
    "                    optimizer = optimizers.Adam(learning_rate = 0.001),\n",
    "                    train_batch_size = None, val_batch_size = None,\n",
    "                    buffer_size = 4096, gradient_accumulation_steps = None,\n",
    "                    run_eagerly = True, verbose = 1):\n",
    "        \"\"\"\n",
    "            This is the function that start the training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pass the input variables to tensorflow default types\n",
    "        x = tf.cast(x, dtype = tf.float32)\n",
    "        t = tf.cast(t, dtype = tf.float32)\n",
    "        delta = tf.cast(delta, dtype = tf.float32)\n",
    "        \n",
    "        # If input is a vector, transform it into a column\n",
    "        if(len(x.shape) == 1):\n",
    "            x = tf.reshape( x, shape = (len(x), 1) )\n",
    "        if(len(t.shape) == 1):\n",
    "            t = tf.reshape( t, shape = (len(t), 1) )\n",
    "        if(len(delta.shape) == 1):\n",
    "            delta = tf.reshape( delta, shape = (len(delta), 1) )\n",
    "\n",
    "        # Salva os dados originais\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.delta = delta\n",
    "\n",
    "        # If no validation step should be taken, training data is the same as validation data\n",
    "        self.x_train, self.t_train, self.delta_train = self.x, self.t, self.delta\n",
    "        self.x_val, self.t_val, self.delta_val = self.x, self.t, self.delta\n",
    "        \n",
    "        # Declara os callbacks do modelo\n",
    "        self.callbacks = [ ]\n",
    "        \n",
    "        if(verbose >= 1):\n",
    "            self.callbacks.append( TqdmCallback(verbose = 0, position = 0, leave = True) )\n",
    "        \n",
    "        # If batch_size is unspecified, set it to be the training size. Note that decreasing the batch size to smaller values, such as 500 for example, has previously lead the\n",
    "        # model to converge too early, leading to a lot of time of investigation. When dealing with neural networks in the statistical models context, we recommend to use a single\n",
    "        # batch in training. Alternatives in the case that the sample is too big might be to consider a \"gradient accumulation\" approach.\n",
    "        self.train_batch_size = train_batch_size\n",
    "        if(self.train_batch_size is None):\n",
    "            self.train_batch_size = self.x_train.shape[0]\n",
    "\n",
    "        self.val_batch_size = val_batch_size\n",
    "        if(self.val_batch_size is None):\n",
    "            self.val_batch_size = self.x_val.shape[0]\n",
    "        \n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        if(self.gradient_accumulation_steps is None):\n",
    "            # The number of batches until the actual weights update (we ensure that the weights are updated only once per epoch, even though we might have multiple batches)\n",
    "            self.gradient_accumulation_steps = int(np.ceil( self.x_train.shape[0] / self.train_batch_size ))\n",
    "\n",
    "        self.compile_model(optimizer = optimizer, learning_rate = 0.001, run_eagerly = run_eagerly)\n",
    "\n",
    "        # Create the training dataset\n",
    "        self.buffer_size = buffer_size\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((self.x_train, self.t_train, self.delta_train))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size = self.buffer_size).batch(self.train_batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        self.fit(\n",
    "            train_dataset,\n",
    "            epochs = epochs,\n",
    "            verbose = 0,\n",
    "            callbacks = self.callbacks,\n",
    "            batch_size = self.train_batch_size,\n",
    "            shuffle = shuffle\n",
    "        )\n",
    "        \n",
    "            \n",
    "    def apply_accumulated_gradients(self):\n",
    "        \"\"\"\n",
    "            Given the proper number of steps for the model to accumulate gradients over time, finally applies gradients to update the model weights.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "abef3356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCLUDING NN PARAMETERS IN THE LIST...\n",
      "VARIABLES MAPPING TO INDEX\n",
      "{'raw_alpha': 0, 'raw_gamma': 1, 'raw_phi1': 2, 'raw_phi2': 3, 'raw_theta': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, 'raw_tau': <tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 2, 3, 4, 5], dtype=int32)>}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e81da2661e4834b9c220ee6575d023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': <Variable path=frailty_model_nn_20/raw_alpha, shape=(), dtype=float32, value=0.0>, 'gamma': <Variable path=frailty_model_nn_20/raw_gamma, shape=(), dtype=float32, value=0.0>, 'phi1': <Variable path=frailty_model_nn_20/raw_phi1, shape=(), dtype=float32, value=0.0>, 'phi2': <Variable path=frailty_model_nn_20/raw_phi2, shape=(), dtype=float32, value=0.0>}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'raw_alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexponential(size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      7\u001b[0m delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbinomial(n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m, size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 239\u001b[0m, in \u001b[0;36mFrailtyModelNN.train_model\u001b[0;34m(self, x, t, delta, epochs, shuffle, optimizer, train_batch_size, val_batch_size, buffer_size, gradient_accumulation_steps, run_eagerly, verbose)\u001b[0m\n\u001b[1;32m    236\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_train))\n\u001b[1;32m    237\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(buffer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_batch_size)\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.17/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[45], line 156\u001b[0m, in \u001b[0;36mFrailtyModelNN.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    155\u001b[0m     nn_pars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 156\u001b[0m     likelihood_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnn_pars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# print(\"Trainable variables:\\n\")\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# print(self.trainable_variables)\u001b[39;00m\n\u001b[1;32m    161\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(likelihood_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[0;32mIn[45], line 131\u001b[0m, in \u001b[0;36mFrailtyModelNN.loss_func\u001b[0;34m(self, variables, nn_output, t, delta)\u001b[0m\n\u001b[1;32m    129\u001b[0m raw_tau \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(nn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvars_to_index[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_tau\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(variables)\n\u001b[0;32m--> 131\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mtf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[43mvariables\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw_alpha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m    132\u001b[0m gamma \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp( variables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_gamma\u001b[39m\u001b[38;5;124m\"\u001b[39m] )\n\u001b[1;32m    133\u001b[0m phi1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp( variables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_phi1\u001b[39m\u001b[38;5;124m\"\u001b[39m] )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'raw_alpha'"
     ]
    }
   ],
   "source": [
    "model = FrailtyModelNN(None, None)\n",
    "\n",
    "model.define_structure()\n",
    "\n",
    "x = np.random.normal(size = 10)\n",
    "t = np.random.exponential(size = 10)\n",
    "delta = np.random.binomial(n = 1, p = 0.5, size = 10)\n",
    "\n",
    "model.train_model(x, t, delta,\n",
    "                  epochs = 10, shuffle = True,\n",
    "                  optimizer = optimizers.Adam(learning_rate = 0.001),\n",
    "                  train_batch_size = None, val_batch_size = None,\n",
    "                  buffer_size = 4096, gradient_accumulation_steps = None,\n",
    "                  run_eagerly = True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "952922e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raw_alpha'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables[0].path.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1de8430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.7739912 ],\n",
       "       [-0.86790764],\n",
       "       [-0.7947914 ],\n",
       "       [ 0.41015804],\n",
       "       [ 0.97464114],\n",
       "       [ 0.04176952],\n",
       "       [ 1.498434  ],\n",
       "       [-0.72059375],\n",
       "       [-0.85633576],\n",
       "       [ 0.35985187]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x[:,None])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyenv 3.10.17)",
   "language": "python",
   "name": "pyenv-3.10.17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
